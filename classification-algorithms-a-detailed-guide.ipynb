{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1263738,"sourceType":"datasetVersion","datasetId":727551},{"sourceId":2047221,"sourceType":"datasetVersion","datasetId":1226038},{"sourceId":2603715,"sourceType":"datasetVersion","datasetId":1582403}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello everyone!\n\n\n<img src=\"https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExeTQ2anQ0NzYwMGl5anliZzczcnFjYjBjbGtkd3FwNnA0OGNncmZjbiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/LoCDk7fecj2dwCtSB3/giphy.webp\">\n\n\nToday we will dive into the world of classification problems in data science. Don’t worry, whether you are an engineer or an English teacher, I have you covered. We will try to understand from everyone’s perspective. So fasten your seat belts and get ready for this amazing journey! Assuming that you have some knowledge about the basics of AI, data science, and machine learning, we will skip the introductions and get straight to the action. We will work with machine learning algorithms to address some classic classification problems. Let’s start with the first step\n\nReady? Let’s get started!","metadata":{}},{"cell_type":"markdown","source":"\n<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*6LMDb34Pv4sTkt-55Zgo9A.png\">","metadata":{}},{"cell_type":"markdown","source":"\n# Problem Definition\n\n*The first step is always the same, no matter what type of problem you're working on: you must identify your problem, understand your dataset, and come up with a rock-solid game plan.*\n\nToday, we focus on a very serious issue: cardiovascular diseases (CVDs). Did you know that CVDs are the first leading cause of death globally?\n\nThey account for an estimated 17.9 million lives lost in a year, representing a gigantic 31% of all the deaths worldwide. And get this: four out of five of those deaths are due to heart attacks and strokes, with a third occurring prematurely in people under 70. CVDs usually lead to heart failure, which is where our dataset comes into play.\n\nWe possess a dataset of 11 important features that help predict the likelihood of heart disease. Early detection and management are crucial for anyone suffering from or at high risk of cardiovascular disease, such as hypertension, diabetes, hyperlipidemia, or an already established disease. That is exactly where a well-trained machine learning model can make the big difference. We now possess all the features and data required for a database that predicts heart diseases, so let's tuck in and start preparing the model bit by bit. Our dataset and our problem are the supervised problems. Now let's examine the data.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*urEV9Jr7qoP--9Qa.png\">","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries\n\n(((: After identifying the problem, the first thing we need to do is import the necessary libraries so that we can start working on the tasks :)))","metadata":{}},{"cell_type":"markdown","source":"* If some libraries don't work, it means Kaggle hasn't updated them yet. I suggest you download and install them manually just this once.","metadata":{}},{"cell_type":"code","source":"!pip install fairlearn\n!pip install lime\n!pip install xgboost\n!pip install catboost\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:58:12.123791Z","iopub.execute_input":"2024-08-26T22:58:12.124445Z","iopub.status.idle":"2024-08-26T22:59:19.987367Z","shell.execute_reply.started":"2024-08-26T22:58:12.124376Z","shell.execute_reply":"2024-08-26T22:59:19.985926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_rows\",None)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Fill Data \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import IsolationForest\n\n# Train-Test \nfrom sklearn.model_selection import train_test_split\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# Machine Algorithm\n\n###### TREE Algorithm\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n###### Non Tree Algorithm\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n\n#Hyperparameter optimizations\nfrom sklearn.experimental import enable_halving_search_cv \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, HalvingRandomSearchCV\nfrom skopt import BayesSearchCV\n\n# Model evaluation\nfrom sklearn.metrics import classification_report, f1_score ,recall_score, roc_auc_score\n\n# Fairness Metrics bias mitigation\nfrom fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n\n\n# Model Interpretability\nimport shap\nimport lime\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Probability Calibration\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.calibration import CalibratedClassifierCV\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-26T22:59:19.990041Z","iopub.execute_input":"2024-08-26T22:59:19.990486Z","iopub.status.idle":"2024-08-26T22:59:29.869043Z","shell.execute_reply.started":"2024-08-26T22:59:19.990440Z","shell.execute_reply":"2024-08-26T22:59:29.867725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's load the data\ndf = pd.read_csv(\"/kaggle/input/heart-failure-prediction/heart.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:59:29.870716Z","iopub.execute_input":"2024-08-26T22:59:29.871566Z","iopub.status.idle":"2024-08-26T22:59:29.901867Z","shell.execute_reply.started":"2024-08-26T22:59:29.871507Z","shell.execute_reply":"2024-08-26T22:59:29.900545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n\n<img src=\"https://www.researchgate.net/profile/Mahmoud_Elansary2/publication/352546274/figure/fig4/AS:1036518353289217@1624136641643/Exploratory-Data-Analysis-EDA-steps-source-7.png\">\n\n***Alright, folks, it’s time to roll up our sleeves and dive into Exploratory Data Analysis (EDA)! Think of EDA as the detective work we do to get familiar with our dataset. It’s all about understanding what’s going on with the data—finding patterns, spotting any weird anomalies, and identifying those all-important features. Basically, EDA helps us get the lay of the land so we can choose the best model and perform deeper analyses later on.***\n\nHere’s the game plan for our EDA:\n\nData Identification: First things first, we need to get to know our data. What kind of data are we dealing with? What do the features represent? Let’s answer these questions before we dive deeper.\n\nData Cleaning: Next, we clean up our data. Think of this like tidying up your room—getting rid of anything that doesn’t belong, filling in the gaps, and making sure everything is in the right place.\n\nData Visualization: Now, we make our data speak visually. We’ll create charts, graphs, and plots to see if there are any obvious patterns or trends.\n\nData Transformation: Sometimes, our data needs a little makeover. This could mean normalizing values, encoding categories, or other transformations that make our data easier to work with.\n\nFeature Analysis: Here, we dig into each feature to understand its impact. Are some features more important than others? We’ll find out!\n\nCorrelation Analysis: Lastly, we check out how our features relate to each other. Are there strong correlations we should be aware of? This step helps us understand these relationships better.\n\nAlright, let's kick things off with Data Identification and see what we’re working with!","metadata":{}},{"cell_type":"markdown","source":"## Preview\n\nBy using info() we can predict the fullness of the features, data quantity, data type and content in the data set.\n\nLet's do a short preview later.","metadata":{}},{"cell_type":"code","source":"# Let's examine the first few rows and basic statistics of the dataset\ndf.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:59:29.904865Z","iopub.execute_input":"2024-08-26T22:59:29.905440Z","iopub.status.idle":"2024-08-26T22:59:29.946927Z","shell.execute_reply.started":"2024-08-26T22:59:29.905360Z","shell.execute_reply":"2024-08-26T22:59:29.945171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't have null values","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T22:59:29.948445Z","iopub.execute_input":"2024-08-26T22:59:29.948844Z","iopub.status.idle":"2024-08-26T22:59:29.976088Z","shell.execute_reply.started":"2024-08-26T22:59:29.948802Z","shell.execute_reply":"2024-08-26T22:59:29.974618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Properties\n\nWhen we look at the features of our dataset, demographic data and factors affecting heart disease are considered. Let's examine the unique values ​​and distributions of each and move on to preparation.\n\n* Age: age of the patient [years]\n* Sex: sex of the patient [M: Male, F: Female]\n* ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n* RestingBP: resting blood pressure [mm Hg]\n* Cholesterol: serum cholesterol [mm/dl]\n* FastingBS: fasting blood sugar [1: if FastingBS > 120 mg/dl, 0: otherwise]\n* RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n* MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n* ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n* Oldpeak: oldpeak = ST [Numeric value measured in depression]\n* ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n* HeartDisease: output class [1: heart disease, 0: Normal]","metadata":{}},{"cell_type":"markdown","source":"**If you divide your work into pieces, you can manage it more easily, so we will evaluate numerical and non-numerical categories separately. First of all, we need to perform distribution and cardinal data control analysis for demographic characteristics, and distribution and outlier detection for numerical characteristics.**\n\n## Cardinal Data and Missing Data Control\n\n\n**Handling Missing Values:**\n\nMissing data can create problems in the analysis. These data can be filled in, removed, or processed according to a specific method.\n\n**Removing Inconsistencies:**\n\nInconsistencies in the data set can be caused by data entry errors and can distort the analysis result. For example, entering the same data in different formats (for example, as \"High School\" and \"HS\") can lead to inconsistencies.\nSuch inconsistencies should be eliminated and the data should be standardized.\n\n**Correcting Incorrect Data Types:**\n\nThe types of data in the columns (numeric, categorical, date, etc.) may be incorrect. These incorrect types should be converted to the correct data type.\n\nFor example, if a numeric column is defined as text, it should be converted to numeric values.\n\nIn addition, we need to convert each data type to numeric values ​​before building our model because the algorithms do not detect sentences or words.\n\n**Detecting and Correcting Outliers:**\n\nOutliers are data that are significantly different from other observations in the data set. These values ​​may be due to incorrect measurement, data entry errors, or unexpected events.\n\nOutliers should be handled carefully as they may cause misleading results in the analysis.\n\n**Cardinal data analysis:**\n\nCardinal Data generally refers to categorical data that has a very high number of unique values. For example, a customer ID or IP addresses may be cardinal data.\n\n**It May Affect the Performance of Models:**\n\nSince cardinal data contains a large number of unique categories, it is difficult to use this data directly in modeling. High cardinality may cause the model to over-learn (overfitting).\n\n**Consumes Memory and Computational Power:**\n\nHigh cardinality creates large data matrices. This causes an increase in memory usage and computational time.\n\n**Noise and Meaninglessness:**\n\nCardinal data may sometimes contain too much noise and become meaningless.","metadata":{}},{"cell_type":"code","source":"categorical_features = [col for col in df.columns if df[col].dtype in ['O','bool_']]\n\npd.DataFrame({  \n              'cardinality': df[categorical_features].nunique(),\n             })","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:00:13.473245Z","iopub.execute_input":"2024-08-26T23:00:13.473878Z","iopub.status.idle":"2024-08-26T23:00:13.495784Z","shell.execute_reply.started":"2024-08-26T23:00:13.473817Z","shell.execute_reply":"2024-08-26T23:00:13.494176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to choose a threshold value for cardinal data. If it exceeds the threshold value we choose, we can either delete it or perform rare encoding operations. (Rare encoding briefly prevents cardinalization and overfitting.)\n* Our threshold value:\n* For small data sets: 5-10 unique values\n* For medium-sized data sets: 10-50 unique values\n* For large data sets: 50-100 unique values\n\nWe did not detect any cardinal data or null values ​​in the categorical variables in the dataset. This is good while developing the model.\n\nNow let's visualize the distributions of categorical data and perform analysis for the next steps.","metadata":{}},{"cell_type":"markdown","source":"## Categoric Columns\n\n### Visualize","metadata":{}},{"cell_type":"code","source":"# We visualize each categorical variable with HeartDisease\nplt.figure(figsize=(20, 15))\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(data=df, x=feature, hue='HeartDisease')\n    plt.title(f'{feature} vs HeartDisease')\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:00:38.073456Z","iopub.execute_input":"2024-08-26T23:00:38.074103Z","iopub.status.idle":"2024-08-26T23:00:39.628237Z","shell.execute_reply.started":"2024-08-26T23:00:38.074048Z","shell.execute_reply":"2024-08-26T23:00:39.626601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\ndf['HeartDisease'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90)\nplt.title('Distribution of HeartDisease')\nplt.ylabel('')  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:22:38.930051Z","iopub.execute_input":"2024-08-26T23:22:38.930662Z","iopub.status.idle":"2024-08-26T23:22:39.143135Z","shell.execute_reply.started":"2024-08-26T23:22:38.930612Z","shell.execute_reply":"2024-08-26T23:22:39.141234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Let's examine the relationship between each categorical variable and HeartDisease.\ncategorical_analysis = {}\n\nfor feature in categorical_features:\n    cross_tab = pd.crosstab(df[feature], df['HeartDisease'], normalize='index')\n    categorical_analysis[feature] = cross_tab\n\n\n# Let's convert the analysis results of categorical variables into a DataFrame\ncategorical_analysis_df = pd.concat(categorical_analysis.values(), keys=categorical_analysis.keys())\ncategorical_analysis_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:00:45.374578Z","iopub.execute_input":"2024-08-26T23:00:45.376341Z","iopub.status.idle":"2024-08-26T23:00:45.454948Z","shell.execute_reply.started":"2024-08-26T23:00:45.376265Z","shell.execute_reply":"2024-08-26T23:00:45.453425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"categorical distributions are generally neither good nor bad, of course our data is insufficient, which is one reason for this\nThere are factors that will reduce our accuracy rate in model building and later, or situations that may have bias. We can use class weighting, sampling techniques or more advanced modeling methods to avoid these situations. Keep in mind\n\n##### Sex:\n\n26% of women have heart disease.\n63% of men have heart disease.\n\n##### ChestPainType:\n\n79% of those who are asymptomatic (ASY) have heart disease.\n14% of those with atypical angina (ATA) have heart disease.\n35% of those with non-anginal pain (NAP) have heart disease.\n43% of those with typical angina (TA) have heart disease.\n\n##### FastingBS (Fasting Blood Sugar):\n\n48% of patients with fasting blood sugar below 120 mg/dl have heart disease.\n79% of patients with fasting blood sugar above 120 mg/dl have heart disease.\n\n##### RestingECG (Resting ECG Results):\n\n56% of patients with left ventricular hypertrophy (LVH) have heart disease.\n52% of patients with normal ECG results have heart disease.\n66% of patients with ST-T wave abnormalities have heart disease.\n\n##### ExerciseAngina (Exercise-Triggered Angina):\n\n35% of patients without exercise-induced angina have heart disease.\n85% of patients with exercise-induced angina have heart disease.\n\n##### ST_Slope (ST Segment Slope):\n\n78% of patients with downsloping ST segment have heart disease.\n83% of patients with a flat-sloping ST segment have heart disease.\n20% of patients with an upward-sloping ST segment have heart disease.\n\nAfter obtaining the analysis, let's move on to our numerical features.","metadata":{}},{"cell_type":"markdown","source":"## Numeric Columns","metadata":{}},{"cell_type":"code","source":"# Let's select columns to visualize numeric data\nnumeric_features = [col for col in df.columns if df[col].dtype not in ['O','bool_'] and col != 'target']\n\n# Let's calculate the correlation between numerical features\ncorrelation_matrix = df[numeric_features].corr()\n\n# Let's create a heatmap for the correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix of Numerical Features')\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:24:05.819440Z","iopub.execute_input":"2024-08-26T23:24:05.820087Z","iopub.status.idle":"2024-08-26T23:24:06.402267Z","shell.execute_reply.started":"2024-08-26T23:24:05.820028Z","shell.execute_reply":"2024-08-26T23:24:06.400933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, looking at the correlation between our numerical features will tell us a lot.\n\nGenerally speaking, there is no multicorrelation, which is good for us in modeling our dataset because high correlation between variables reduces the stability and interpretability of the model.\n\n\nThere is a positive correlation between Age and HeartDisease (0.28). In other words, the risk of heart disease increases as age increases.\n\nThere is a weak positive correlation between RestingBP and HeartDisease (0.11).\n\nThere is a weak negative correlation between Cholesterol and HeartDisease (-0.23).\n\nThere is a moderate negative correlation between MaxHR and HeartDisease (-0.40). This indicates that individuals with higher maximum heart rates may have a lower risk of heart disease.\n\nThere is a moderate positive correlation between Oldpeak and HeartDisease (0.40). This indicates that the risk of heart disease may increase if ST segment depression is higher.","metadata":{}},{"cell_type":"code","source":"\n# Let's visualize these numerical features with histograms\n\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(numeric_features, 1):\n    plt.subplot(3, 3, i)\n    sns.histplot(df[feature], kde=True, bins=30)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:01:39.970978Z","iopub.execute_input":"2024-08-26T23:01:39.972819Z","iopub.status.idle":"2024-08-26T23:01:43.187546Z","shell.execute_reply.started":"2024-08-26T23:01:39.972746Z","shell.execute_reply":"2024-08-26T23:01:43.186197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pairplot to visualize relationships between numerical features\nsns.pairplot(df[numeric_features], hue=\"HeartDisease\", diag_kind=\"kde\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:25:03.702790Z","iopub.execute_input":"2024-08-26T23:25:03.704483Z","iopub.status.idle":"2024-08-26T23:25:19.898122Z","shell.execute_reply.started":"2024-08-26T23:25:03.704399Z","shell.execute_reply":"2024-08-26T23:25:19.896451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cholesterol Analysis\n\nLooking at the scatter plot, we can see that the cholesterol data is spread out over a wide range, but there’s an interesting peak near zero. This peak might suggest some unrealistic values in our dataset—after all, having a cholesterol level near zero isn’t possible!\n\n\nWhen we check out the boxplot, it seems like there’s not a huge difference in cholesterol levels between individuals with and without heart disease. However, those zero values stand out and could be considered abnormal.\n\n\n* The average cholesterol value is 198.8 mg/dl, which is within a reasonable range for most people.\n* However, we have a minimum value reported as 0, which is not realistic for cholesterol levels. This could indicate errors or missing data in the dataset.\n\nGiven these findings, it’s likely that we’ll treat the zero or abnormally low cholesterol values as incorrect or missing data. We’ll need to clean up our dataset by addressing these inaccuracies to ensure our analysis is based on reliable data.\n\nOldpeak Analysis\n\nFor the Oldpeak data, the scatter plot shows that most values are clustered between 0 and 1.5, but there are some that fall outside this range, indicating a broader distribution.\n\n\nThe boxplot reveals a significant difference in Oldpeak values between individuals with and without heart disease. Generally, those with heart disease tend to have higher Oldpeak values.\n\n\n\n* The mean Oldpeak value is 0.887, with a standard deviation of 1.067, showing some variability in the data.\n* The minimum Oldpeak value is reported as -2.6, which doesn’t make sense because Oldpeak cannot be negative.\n* The maximum value is 6.2, which is quite high. This might be an outlier, so we’ll need to investigate further in our visualizations.\n\nWe’ll need to dig deeper into these unusual values, especially the negative and very high Oldpeak figures, to determine if they’re errors or valid outliers. By refining our dataset, we can improve the accuracy of our analysis and ensure our models are robust.","metadata":{}},{"cell_type":"code","source":"# Let's show the relationships between numerical and categorical features with violin plot\n\nplt.figure(figsize=(20, 15))\n\nfor i, numeric_feature in enumerate(numeric_features):\n    for j, categorical_feature in enumerate(categorical_features):\n        plt.subplot(len(numeric_features), len(categorical_features), i * len(categorical_features) + j + 1)\n        sns.violinplot(x=categorical_feature, y=numeric_feature, data=df)\n        plt.title(f'{numeric_feature} vs {categorical_feature}')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:02:03.573246Z","iopub.execute_input":"2024-08-26T23:02:03.573842Z","iopub.status.idle":"2024-08-26T23:02:13.523083Z","shell.execute_reply.started":"2024-08-26T23:02:03.573784Z","shell.execute_reply":"2024-08-26T23:02:13.521532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploring Relationships Between Features**\nLet’s dive deeper into our data to explore how different features are related. Understanding these relationships can really help us make smarter decisions when building our model. Here’s what we’ve discovered so far:\n\n**Relationship Between Sex and Numerical Features:**\n\n***Age:*** While age itself doesn’t directly depend on gender, combining these two can reveal some interesting patterns. For instance, the risk of heart disease might vary across age groups differently for men and women. It’s definitely something worth looking into!\n\n***Maximum Heart Rate (MaxHR):*** We might notice that maximum heart rate differs between genders. This could suggest that when building our model, we might need to consider gender as a segment to account for these differences effectively.\n\n**Relationship Between Chest Pain Type and Numerical Features:**\n\n***Cholesterol and Resting Blood Pressure (RestingBP):*** TThere appear to be significant differences in cholesterol levels and blood pressure depending on the type of chest pain someone experiences. These differences could be crucial for assessing the risk of heart disease, making them important features to focus on.\n\n***Oldpeak:*** Oldpeak, which refers to ST segment depression after exercise, can vary based on the type of chest pain. Understanding these variations is super helpful for predicting heart disease in our models, as it provides deeper insights into the patient’s condition.\n\n\n\n**Relationship Between Exercise-Induced Angina and Numerical Features:**\n\n***MaxHR and Oldpeak:*** Exercise-induced angina seems closely connected with both maximum heart rate and post-exercise ST segment depression (Oldpeak). These relationships are key for assessing the risk of heart disease progression, making them valuable features for our model.\n\n**Relationship Between Fasting Blood Sugar (FastingBS) and Numerical Features:**\n\n***Cholesterol and RestingBP:*** Individuals with high fasting blood sugar often have different cholesterol and blood pressure levels. This information is vital for understanding the link between diabetes and heart disease, which could help refine our model.\n\n**Relationship Between Heart Disease and Numerical Features:**\n\n***Age, MaxHR, Cholesterol, RestingBP, Oldpeak:*** These features show quite a bit of variation between those with heart disease and those without. Because of this, they’re likely to be key predictors in our model and should be considered carefully.\n### Useful Inferences for Next Steps\n**Segmentation and Grouping:** By segmenting our data based on categorical variables like gender, chest pain type, and exercise-induced angina, we can get a clearer picture of the dataset. Treating these segments separately might lead to better modeling outcomes and more accurate predictions.\n\n**Feature Selection:** Observing how numerical features like MaxHR, Cholesterol, RestingBP, and Oldpeak change across different categories can guide us in selecting the most relevant features for our model. This step is crucial for building a strong, predictive model that accurately reflects the underlying data.\n\nLet’s use these insights to refine our approach and create the most effective model possible!","metadata":{}},{"cell_type":"code","source":"# First, let's identify the highest physiological columns for RestingBP and Cholesterolcorrelation_matrix = df[numeric_features].corr()\ncorrelation_matrix\n# Let's examine the correlations of RestingBP, Cholesterol and Oldpeak columns with other columns\nrestingbp_corr = correlation_matrix['RestingBP'].sort_values(ascending=False)\ncholesterol_corr = correlation_matrix['Cholesterol'].sort_values(ascending=False)\nOldpeak_corr = correlation_matrix['Oldpeak'].sort_values(ascending=False)\nrestingbp_corr, cholesterol_corr,Oldpeak_corr\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:02:21.984267Z","iopub.execute_input":"2024-08-26T23:02:21.984808Z","iopub.status.idle":"2024-08-26T23:02:22.000335Z","shell.execute_reply.started":"2024-08-26T23:02:21.984759Z","shell.execute_reply":"2024-08-26T23:02:21.998865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's detect zero values in RestingBP and Cholesterol\nanomalous_restingbp = df['RestingBP'] == 0\nanomalous_cholesterol = df['Cholesterol'] == 0\nimputer = SimpleImputer(strategy='median')\n\n# For RestingBP: Let's fill in the zero values by grouping with Age and Oldpeak\ndf.loc[anomalous_restingbp, 'RestingBP'] = df.groupby(['Age', 'Oldpeak'])['RestingBP'].transform(\n    lambda x: x.replace(0, x.median())\n)\n\n# For Cholesterol: Let's fill in the zero values by grouping with MaxHR and RestingBP\ndf.loc[anomalous_cholesterol, 'Cholesterol'] = df.groupby(['MaxHR', 'RestingBP'])['Cholesterol'].transform(\n    lambda x: x.replace(0, x.median())\n)\n\n\n# Make values less than 0 NaN in Oldpeak\ndf.loc[df['Oldpeak'] < 0, 'Oldpeak'] = float('nan')\ndf['Oldpeak'] = imputer.fit_transform(df[['Oldpeak']])\n\n# make the remaining values nan and fill them with interpolation method\ndf['Cholesterol'].replace(0, np.nan, inplace=True)\n# Let's fill the values that cannot be filled after grouping with the median\ndf['Cholesterol'] = df['Cholesterol'].interpolate(method='linear')\n\n# Let's check the filled values again\nrestingbp_summary_after = df['RestingBP'].describe()\ncholesterol_summary_after = df['Cholesterol'].describe()\noldpeak_summary_after = df['Oldpeak'].describe()\n\nrestingbp_summary_after, cholesterol_summary_after ,oldpeak_summary_after\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:02:32.391443Z","iopub.execute_input":"2024-08-26T23:02:32.392050Z","iopub.status.idle":"2024-08-26T23:02:32.890151Z","shell.execute_reply.started":"2024-08-26T23:02:32.391970Z","shell.execute_reply":"2024-08-26T23:02:32.888824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RestingBP graph - After filling\nplt.figure(figsize=(15, 10))\nplt.subplot(3, 1, 1)\nsns.histplot(df['RestingBP'], kde=True, bins=30, color='blue')\nplt.title('RestingBP Distribution (After Imputation)')\nplt.xlabel('RestingBP')\nplt.ylabel('Frequency')\n\n# Cholesterol chart - After filling\nplt.subplot(3, 1, 2)\nsns.histplot(df['Cholesterol'], kde=True, bins=30, color='green')\nplt.title('Cholesterol Distribution (After Imputation)')\nplt.xlabel('Cholesterol')\nplt.ylabel('Frequency')\n\n# Cholesterol chart - After filling\nplt.subplot(3, 1, 3)\nsns.histplot(df['Oldpeak'], kde=True, bins=30, color='orange')\nplt.title('Oldpeak Distribution (After Imputation)')\nplt.xlabel('Cholesterol')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:02:44.035197Z","iopub.execute_input":"2024-08-26T23:02:44.035782Z","iopub.status.idle":"2024-08-26T23:02:45.336573Z","shell.execute_reply.started":"2024-08-26T23:02:44.035726Z","shell.execute_reply":"2024-08-26T23:02:45.334892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Why Detect Outliers?\n\n<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_anomaly_comparison_001.png\">\n\n\nNow let's talk about outliers. These are quirky data points that seem not to sit very well with the rest of them. Outlier detection is a key step in data preparation and for a number of reasons, including the following: Enhanced model performance means more realistic interactions. Outliers can wreak havoc on the learning process of your model. Let's consider linear regression. As has already been said, this model does not take into account the contribution of each data point separately. Everything becomes messed up when an outlier is extremely valued. This might lead your model to assign weird, oversized coefficients, reducing the overall accuracy of your model. Outliers make models give very strange answers and are therefore very unreliable.\n\nOverfitting: Another problem of outliers is that they can lead to overfitting. If your model fits the unusual data points too well, it ends up performing well on training but failing to generalize to new, unseen data. It's sort of like training for a marathon by running only downhill: you'll nail the downhill, but as soon as you hit a flat or uphill section, you're in trouble. Equally, overfitting due to outliers can lead to a significant drop in performance of your model in the real world.\n\nDetection and handling of outliers can help to make our models more accurate, generalizable, and robust in nature. Let's therefore not skip this important step; it is key to creating reliable machine learning models!","metadata":{}},{"cell_type":"code","source":"\n\n# Set up the visualizations for each column with potential outliers\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n\n# Plotting each potential outlier column\nsns.boxplot(x=df['Cholesterol'], ax=axes[0, 0]).set_title('Cholesterol')\nsns.boxplot(x=df['MaxHR'], ax=axes[1, 0]).set_title('MaxHR')\nsns.boxplot(x=df['Oldpeak'], ax=axes[1, 1]).set_title('Oldpeak')\nsns.boxplot(x=df['RestingBP'], ax=axes[2, 0]).set_title('RestingBP')\n\n# Hide the last subplot as it's not needed\naxes[2, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:03:04.106518Z","iopub.execute_input":"2024-08-26T23:03:04.107146Z","iopub.status.idle":"2024-08-26T23:03:05.221190Z","shell.execute_reply.started":"2024-08-26T23:03:04.107088Z","shell.execute_reply":"2024-08-26T23:03:05.219800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating age groups\ndf['AgeGroup'] = pd.cut(df['Age'], bins=[20, 40, 60, 80], labels=['20-40', '40-60', '60-80'])\n\n# Outlier detection and replacement function\ndef replace_outliers_corrected(df, group_column, target_columns):\n    for col in target_columns:\n        for group_name, group in df.groupby(group_column):\n            iso_forest = IsolationForest(contamination=0.05, random_state=42)\n            outliers = iso_forest.fit_predict(group[[col]])\n            median_value = group[col].median()\n            df.loc[group.index[outliers == -1], col] = median_value\n    return df\n\n# Let's specify the relevant columns\noutlier_columns = ['RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n\n# Substituting outliers within groups\ndf = replace_outliers_corrected(df, 'AgeGroup', outlier_columns)\n\ndf.drop(\"AgeGroup\",axis=1,inplace= True)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:03:14.477991Z","iopub.execute_input":"2024-08-26T23:03:14.479371Z","iopub.status.idle":"2024-08-26T23:03:18.746987Z","shell.execute_reply.started":"2024-08-26T23:03:14.479296Z","shell.execute_reply":"2024-08-26T23:03:18.745657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering \n\n\n<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*kCuNfifKaF-qhpwwKrESZQ.png\">\n\n**Feature Engineering:** \nThe Secret Sauce of Machine Learning Feature engineering is like adding secret ingredients to your grandmother's famous cookie recipe: it's all about creating something extraordinary from the ordinary. The art of feature engineering refers to crafting new, predictive features from raw data in such a way that boosts the learning capacity of your model and leads to better results.","metadata":{}},{"cell_type":"code","source":"# Creating interaction terms\n\n# df['Age_Sex'] = df['Age'] * df['Sex']\n# df['ChestPain_MaxHR'] = df['ChestPainType'] * df['MaxHR']\n# df['Oldpeak_ExerciseAngina'] = df['Oldpeak'] * df['ExerciseAngina']\n\n# df.drop(labels=['Age','Sex','ChestPainType','MaxHR','Oldpeak','ExerciseAngina'],axis=1,inplace = True)\n# Display the updated dataframe with new interaction terms\n# print(df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:03:48.108748Z","iopub.execute_input":"2024-08-26T23:03:48.109412Z","iopub.status.idle":"2024-08-26T23:03:48.116369Z","shell.execute_reply.started":"2024-08-26T23:03:48.109354Z","shell.execute_reply":"2024-08-26T23:03:48.114687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Not Always Needed, But Often Beneficial:** In some rare cases, a dataset might be perfectly clean and complete—like finding all the ingredients for your favorite recipe prepped and ready. But most of the time, a little feature engineering goes a long way.\n\n\n**Model Complexity:** Including interaction terms can increase model complexity. If interaction terms do not contribute significantly to model performance (e.g., improve accuracy, recall, precision, etc.), it may be better to remove them to avoid overfitting and simplify the model.\n\n**Interpretability:** Depending on the model, interaction terms can make it difficult to interpret results. If interpretability is important, you may want to avoid using these terms unless they provide significant predictive value.\n\n**Model Performance Evaluation:**\n\nEvaluate the performance of the model on the validation set with and without these interaction terms. If the interaction terms significantly improve the model, keep them; otherwise, consider removing them.\n","metadata":{}},{"cell_type":"markdown","source":"# One Hot Encoding\n\n<img src=\"https://www.researchgate.net/profile/Fatemeh-Davoudi-Kakhki/publication/344409939/figure/fig1/AS:940907041918978@1601341128930/An-example-of-one-hot-encoding.png\">\n\n**What is One-Hot Encoding?**\n\nLet’s talk about One-Hot Encoding—a super handy technique when you're working with categorical data in machine learning. Categorical data is basically data that falls into specific categories or classes, like colors (red, blue, green) or types of fruit (apple, banana, cherry). The problem is, machine learning models don’t really know what to do with words or labels—they need numbers to crunch!\n\nThat’s where One-Hot Encoding comes in. It’s a way to convert these categories into a numerical format that our models can understand. Here’s how it works: One-Hot Encoding creates new columns (or features) for each category, filling them with binary values—0s and 1s. Each row in your data will have a ‘1’ in the column of the category it belongs to and ‘0’s in all other category columns.\n\n","metadata":{}},{"cell_type":"code","source":"\n# Let's code categorical variables using one-hot coding method\ndf_encoded = pd.get_dummies(df, columns=categorical_features, dtype='int64',drop_first=True)\n\n# Let's show the first few lines\ndf_encoded.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:04:12.489585Z","iopub.execute_input":"2024-08-26T23:04:12.490988Z","iopub.status.idle":"2024-08-26T23:04:12.523236Z","shell.execute_reply.started":"2024-08-26T23:04:12.490933Z","shell.execute_reply":"2024-08-26T23:04:12.521912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Splitting","metadata":{}},{"cell_type":"code","source":"# Let's separate the target variable and properties\nX_tree = df_encoded.drop('HeartDisease', axis=1)\ny_tree = df_encoded['HeartDisease']\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:04:37.029987Z","iopub.execute_input":"2024-08-26T23:04:37.030540Z","iopub.status.idle":"2024-08-26T23:04:37.039643Z","shell.execute_reply.started":"2024-08-26T23:04:37.030496Z","shell.execute_reply":"2024-08-26T23:04:37.038134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's split the data into training and test sets\nX_train_tree, X_test_tree, y_train_tree, y_test_tree = train_test_split(X_tree, y_tree, test_size=0.2, random_state=42)\n\n# Let's check the sizes of training and test sets\nX_train_tree.shape, X_test_tree.shape, y_train_tree.shape, y_test_tree.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:04:43.027543Z","iopub.execute_input":"2024-08-26T23:04:43.028098Z","iopub.status.idle":"2024-08-26T23:04:43.043332Z","shell.execute_reply.started":"2024-08-26T23:04:43.028049Z","shell.execute_reply":"2024-08-26T23:04:43.042022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Non-Normalized Model\n\n\n<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/0*U0rcW7XrdHpvI0hU.jpeg\">\n\n**Machine Learning Algorithms That Don’t Sweat the Small Stuff: Normalization and Standardization**\n\nWhen working with machine learning, you often hear about the need to normalize or standardize your data—basically, making sure all your features are on a similar scale. This is super important for many algorithms that are sensitive to the scale of data. But guess what? Not all algorithms need this kind of preprocessing! Some algorithms are scale-independent and work just fine without normalization or standardization. Let’s look at a few of these:\n\n***Decision Trees:***\n\nDecision trees are like those people who don't care about the fancy stuff—they’re straightforward and to the point. They split the data based on feature values, regardless of the scale. Whether your features range from 1 to 10 or 1 to 10,000, it doesn’t matter. Decision trees operate independently of feature scales, so you can skip normalization and standardization here.\n\n***Random Forests:***\n\nRandom forests are basically a bunch of decision trees working together, like a team of independent thinkers. Since each tree in a random forest handles features just like a single decision tree, this algorithm is also scale-independent. No need to worry about the scales of your features with random forests—they don’t mind.\n\n***Naive Bayes:***\n\nNaive Bayes takes a different approach. It’s based on the assumption of independence between features and class labels, which means it doesn’t consider the relationship between features themselves. Because of this, the algorithm isn’t affected by the scales of the data. Whether your features are measured in grams or tons, Naive Bayes will handle them just the same.\n\n***Boosting Algorithms (e.g., AdaBoost, Gradient Boosting, XGBoost, CatBoost):***\n\nBoosting algorithms are like decision trees on steroids—they use an ensemble of trees to improve prediction accuracy. Since these algorithms rely on decision trees, they inherit the same scale independence. So, just like with decision trees and random forests, you don’t need to worry about normalizing or standardizing your data when using boosting algorithms.\n\nSo, if you’re working with any of these algorithms, you can save yourself some time and skip the normalization or standardization steps. Just dive right into building your model!","metadata":{}},{"cell_type":"code","source":"# Let's define the models tree\nmodels = {    \n    'NaiveBayes': GaussianNB(),\n    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42),\n    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n    'CatBoostClassifier': CatBoostClassifier(random_state=42, verbose=0)\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:04:57.709732Z","iopub.execute_input":"2024-08-26T23:04:57.710304Z","iopub.status.idle":"2024-08-26T23:04:57.722676Z","shell.execute_reply.started":"2024-08-26T23:04:57.710251Z","shell.execute_reply":"2024-08-26T23:04:57.721100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Tuning\n\n<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*axPdc1zynCHtW__d\">\n\n# Hyperparameters: The Secret Sauce of Model Optimization\nHyperparameters are like the secret ingredients in your favorite recipe—they need to be just right to make your machine learning models perform their best. Unlike regular parameters that the model learns on its own, hyperparameters are set by you and need to be fine-tuned to get the optimal results.\n\nThere are plenty of methods out there for hyperparameter optimization, but instead of diving into each one, let’s hit some key points to get you started:\n\n**General Strategy:**\n\nLimited Time and Resources? Try Random Search: If you're working with limited time or computational resources, Random Search is usually a good place to start. It explores the hyperparameter space more broadly and can often find good results without too much effort.\n\nLooking for Something More Sophisticated? Go for Bayesian Optimization or Hyperband: If you're working with complex models and want to get the best results with fewer trials, methods like Bayesian Optimization or Hyperband are great choices. They’re more efficient and can zero in on the optimal hyperparameters faster.\n\nSimple Models? Stick with Grid Search: For simpler models, Grid Search will usually do the trick. It tests every possible combination of hyperparameters, which is great when you don't have too many to consider.\n\nHere’s how we’d approach hyperparameter tuning for some specific models:\n\n**Grid Search for Naive Bayes:**\n\nNaive Bayes models typically have a limited set of hyperparameters to tune. Because of this, Grid Search, which systematically tries every combination of hyperparameters, works really well. It’s straightforward, and since there aren't too many combinations to test, it's quick and efficient.\n\n**Random Search for RandomForestClassifier, AdaBoostClassifier, and DecisionTreeClassifier:**\n\nRandom forests, AdaBoost, and decision trees come with a lot of hyperparameters (like the number of trees in a forest or the maximum depth of a tree). This can make the search space quite large! Random Search is a great option here because it randomly samples from the hyperparameter space, allowing for a fast and efficient search without needing to test every single combination.\n\n**Hyperband for GradientBoostingClassifier and XGBoost:**\n\nFor models like Gradient Boosting and XGBoost, Hyperband is a fantastic choice. These models have a wide range of hyperparameters to tune, and methods like Hyperband can help find the best combination more efficiently than a brute-force approach. Hyperband works by allocating resources dynamically and can quickly discard less promising configurations, saving time and computational power.\n\nBy choosing the right hyperparameter optimization strategy for your model and dataset, you can significantly improve your model's performance without wasting resources. So, whether you’re sticking with the basics or trying out more advanced methods, there’s a strategy that’s perfect for your needs!","metadata":{}},{"cell_type":"code","source":"\n# Hyperparameter optimization functions\n\ndef optimize_naive_bayes(X_train_tree, y_train_tree):\n    param_grid_nb = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n    grid_search_nb = GridSearchCV(estimator=models['NaiveBayes'], param_grid=param_grid_nb, cv=5, scoring='recall')\n    grid_search_nb.fit(X_train_tree, y_train_tree)\n    return grid_search_nb.best_estimator_\n\ndef optimize_random_forest(X_train_tree, y_train_tree):\n    param_dist_rf = {'n_estimators': [100, 200, 500], 'max_depth': [10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n    random_search_rf = RandomizedSearchCV(estimator=models['RandomForestClassifier'], param_distributions=param_dist_rf, n_iter=50, cv=5, scoring='recall', random_state=42)\n    random_search_rf.fit(X_train_tree, y_train_tree)\n    return random_search_rf.best_estimator_\n\ndef optimize_adaboost(X_train_tree, y_train_tree):\n    param_dist_ab = {'n_estimators': [50, 100, 500], 'learning_rate': [0.001, 0.01, 0.1, 1.0]}\n    random_search_ab = RandomizedSearchCV(estimator=models['AdaBoostClassifier'], param_distributions=param_dist_ab, n_iter=50, cv=5, scoring='recall', random_state=42)\n    random_search_ab.fit(X_train_tree, y_train_tree)\n    return random_search_ab.best_estimator_\n\ndef optimize_decision_tree(X_train_tree, y_train_tree):\n    param_dist_dt = {'max_depth': [10, 20, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n    random_search_dt = RandomizedSearchCV(estimator=models['DecisionTreeClassifier'], param_distributions=param_dist_dt, n_iter=50, cv=5, scoring='recall', random_state=42)\n    random_search_dt.fit(X_train_tree, y_train_tree)\n    return random_search_dt.best_estimator_\n\ndef optimize_gradient_boosting(X_train_tree, y_train_tree):\n    param_dist_gb = {'n_estimators': [100, 200, 500], 'learning_rate': [0.001, 0.01, 0.1, 1.0], 'max_depth': [3, 5, 7]}\n    hyperband_gb = HalvingRandomSearchCV(estimator=models['GradientBoostingClassifier'], param_distributions=param_dist_gb, factor=3, random_state=42, cv=5, scoring='recall')\n    hyperband_gb.fit(X_train_tree, y_train_tree)\n    return hyperband_gb.best_estimator_\n\ndef optimize_xgboost(X_train_tree, y_train_tree):\n    param_dist_xgb = {'n_estimators': [100, 200, 300, 400, 500, 1000], 'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0], 'max_depth': [3, 5, 7, 9, 11],\n                      'min_child_weight': [1, 3, 5, 7], 'gamma': [0, 0.1, 0.2, 0.3, 0.4], 'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n                      'reg_alpha': [0, 0.01, 0.1, 1, 10], 'reg_lambda': [0.01, 0.1, 1, 10, 100]}\n    hyperband_xgb = HalvingRandomSearchCV(estimator=models['XGBoost'], param_distributions=param_dist_xgb, factor=3, random_state=42, cv=5, scoring='recall')\n    hyperband_xgb.fit(X_train_tree, y_train_tree)\n    return hyperband_xgb.best_estimator_\n\ndef optimize_catboost(X_train_tree, y_train_tree):\n    param_grid_cb = {'iterations': (100, 1000), 'depth': (4, 10), 'learning_rate': (0.01, 0.3), 'l2_leaf_reg': (1, 10), 'bagging_temperature': (0, 1), 'border_count': (32, 255), 'random_strength': (1e-9, 10)}\n    opt_cb = BayesSearchCV(estimator=models['CatBoostClassifier'], search_spaces=param_grid_cb, n_iter=50, cv=5, scoring='recall', verbose=0, random_state=42)\n    opt_cb.fit(X_train_tree, y_train_tree)\n    return opt_cb.best_estimator_\n\n# Let's optimize the models\nmodels['NaiveBayes'] = optimize_naive_bayes(X_train_tree, y_train_tree)\nmodels['RandomForestClassifier'] = optimize_random_forest(X_train_tree, y_train_tree)\nmodels['AdaBoostClassifier'] = optimize_adaboost(X_train_tree, y_train_tree)\nmodels['DecisionTreeClassifier'] = optimize_decision_tree(X_train_tree, y_train_tree)\nmodels['GradientBoostingClassifier'] = optimize_gradient_boosting(X_train_tree, y_train_tree)\nmodels['XGBoost'] = optimize_xgboost(X_train_tree, y_train_tree)\nmodels['CatBoostClassifier'] = optimize_catboost(X_train_tree, y_train_tree)\n\n# You can print optimized models or save them for later use\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:05:15.221693Z","iopub.execute_input":"2024-08-26T23:05:15.222308Z","iopub.status.idle":"2024-08-26T23:19:16.055873Z","shell.execute_reply.started":"2024-08-26T23:05:15.222251Z","shell.execute_reply":"2024-08-26T23:19:16.054300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models.items()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:16.058562Z","iopub.execute_input":"2024-08-26T23:19:16.058976Z","iopub.status.idle":"2024-08-26T23:19:16.074382Z","shell.execute_reply.started":"2024-08-26T23:19:16.058932Z","shell.execute_reply":"2024-08-26T23:19:16.072906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation and Improvement\n<img src=\"https://media.licdn.com/dms/image/D5612AQGKoVQ8Xhnjzg/article-cover_image-shrink_600_2000/0/1690913948713?e=2147483647&v=beta&t=Ou7PqmMdh9aYgSSD-smMzhB2HCJX1UESmqP8Z9ly5no\">\n\n","metadata":{}},{"cell_type":"markdown","source":"## Understanding the Classification Report\n\nWhen working on classification problems in machine learning and deep learning, the classification report is a handy tool for evaluating how well your model is doing. This report gives you a detailed breakdown of the model's performance across different classes using various metrics. Let's go through the key components of the classification report:\n\n**Precision:**\n\nPrecision tells us how many of the examples that the model predicted as positive were actually positive. In other words, it shows how accurate the positive predictions are. A high precision score means that the model is good at minimizing false positives—meaning it rarely predicts something as positive when it’s not.\n\n**Recall:**\n\nRecall, also known as sensitivity, measures how many of the actual positive examples were correctly predicted by the model. A high recall score indicates that the model is good at capturing all the actual positives, producing few false negatives—meaning it doesn’t miss many positive cases.\n\n**F1-Score:**\n\nThe F1-score provides a balance between precision and recall. It’s the harmonic mean of precision and recall, making it especially useful when you’re dealing with imbalanced classes. If one class is much more common than the other, the F1-score helps ensure that both precision and recall are being considered equally, giving you a better sense of the model's overall performance.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Recall (Sensitivity / True Positive Rate)\n\n*Why is it used?*\nRecall, also known as sensitivity or the true positive rate, is crucial in scenarios where missing a positive case could have serious consequences. Take health-related issues, like cancer detection, for example. In these situations, a false negative—where the model incorrectly predicts a sick patient as healthy—can be extremely dangerous. It means a patient might miss out on early treatment opportunities, which could be life-threatening.\n\n*Why is Recall Important in These Cases?*\nRecall measures how many of the actual positive cases (like patients with cancer) are correctly identified by the model. A high recall means that the model is good at catching nearly all the positive cases, which is exactly what you want in critical health diagnoses. The goal is to minimize the chances of false negatives so that fewer patients are wrongly considered healthy when they are, in fact, ill.\n\n*Especially Used:*\nRecall is particularly important in fields like healthcare, where early and accurate diagnosis is vital. For conditions like cancer, where timely detection can significantly impact treatment outcomes, prioritizing recall helps ensure that as many true positives as possible are caught by the model.","metadata":{}},{"cell_type":"code","source":"\n# Lists to store results\nrecall_scores_test = []\nrecall_scores_train = []\nmodel_names = []\n\n# Get recall scores for each model\nfor name, model in models.items():\n    \n    # Make a prediction (Using trained models in hyperparameter optimization)\n    y_pred_tree = model.predict(X_test_tree)\n    y_pred_train_tree = model.predict(X_train_tree)\n    \n    # Calculate and save recall scores\n    recall_test = recall_score(y_test_tree, y_pred_tree, average='binary')\n    recall_train = recall_score(y_train_tree, y_pred_train_tree, average='binary')\n    \n    recall_scores_test.append(recall_test)\n    recall_scores_train.append(recall_train)\n    model_names.append(name)\n\n# Let's visualize the recall scores on the same graph\nplt.figure(figsize=(12, 6))\n\nplt.plot(model_names, recall_scores_train, marker='o', linestyle='-', color='b', label='Train Recall')\nplt.plot(model_names, recall_scores_test, marker='o', linestyle='-', color='r', label='Test Recall')\n\nplt.xlabel('Models')\nplt.ylabel('Recall')\nplt.title('Train and Test Recall for Different Models')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:16.076124Z","iopub.execute_input":"2024-08-26T23:19:16.076530Z","iopub.status.idle":"2024-08-26T23:19:16.672229Z","shell.execute_reply.started":"2024-08-26T23:19:16.076481Z","shell.execute_reply":"2024-08-26T23:19:16.670742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Naive Bayes: Relatively low recall difference between training and test sets, no memorization.\n\n* Random Forest: Recall in training set is very high (almost 1.0) but recall in test set is lower. Memorized\n\n* Decision Tree: Similarly, recall in training set is high, but this ratio decreases in test set. Memorized\n\n* Gradient Boosting: While there is a perfect recall value (1.0) in training set, it shows very low performance in test set. Memorized\n\n* AdaBoost: There is a more balanced difference between training and test recall values. However, since training set performance is higher, it memorized a little.\n\n* XGBoost: Similar recall values ​​are seen in training and test sets, not memorized.\n\n* CatBoost: Similarly, recall values ​​are close to each other in training and test sets, not memorized","metadata":{}},{"cell_type":"code","source":"comparison_data = []\n\nfor name, model in models.items():\n        \n    # Make a guess\n    y_pred_tree = model.predict(X_test_tree)\n    y_pred_train_tree = model.predict(X_train_tree)\n    \n    # Get classification report\n    report_test = classification_report(y_test_tree, y_pred_tree, output_dict=True)\n    report_train = classification_report(y_train_tree, y_pred_train_tree, output_dict=True)\n    \n    # Let's tabulate the classification report values ​​for each class\n    for label in report_test.keys():\n        if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n            comparison_data.append({\n                'Model': name,\n                'DataSet': 'Test',\n                'Label': label,\n                'Precision': report_test[label]['precision'],\n                'Recall': report_test[label]['recall'],\n                'F1-Score': report_test[label]['f1-score'],\n                'Accuracy': report_test['accuracy']  # Accuracy değerini ekliyoruz\n            })\n            comparison_data.append({\n                'Model': name,\n                'DataSet': 'Train',\n                'Label': label,\n                'Precision': report_train[label]['precision'],\n                'Recall': report_train[label]['recall'],\n                'F1-Score': report_train[label]['f1-score'],\n                'Accuracy': report_train['accuracy']  # Accuracy değerini ekliyoruz\n            })\n\n# Sonuçları bir DataFrame'e dönüştürelim\ncomparison_df = pd.DataFrame(comparison_data)\n\n\n\n# Let's view the DataFrame\ncomparison_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:16.674977Z","iopub.execute_input":"2024-08-26T23:19:16.675419Z","iopub.status.idle":"2024-08-26T23:19:16.908152Z","shell.execute_reply.started":"2024-08-26T23:19:16.675375Z","shell.execute_reply":"2024-08-26T23:19:16.907022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_recall_test = max(comparison_data, key=lambda x: x['Recall'] if (x['DataSet'] == 'Test' and x['Recall']<1) else 0)\n\nprint(\"The best model based test on recall is:\", best_model_recall_test['Model'])\nprint(\"Recall:\", best_model_recall_test['Recall'])\n\nbest_model_recall_train = max(comparison_data, key=lambda x: x['Recall'] if (x['DataSet'] == 'Train' and x['Recall']<1) else 0)\n\nprint(\"The best model based on train recall is:\", best_model_recall_train['Model'])\nprint(\"Recall:\", best_model_recall_train['Recall'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:16.909918Z","iopub.execute_input":"2024-08-26T23:19:16.910333Z","iopub.status.idle":"2024-08-26T23:19:16.919217Z","shell.execute_reply.started":"2024-08-26T23:19:16.910290Z","shell.execute_reply":"2024-08-26T23:19:16.917897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fairness Evaluation\n\n<img src=\"https://dsp700.wordpress.com/wp-content/uploads/2012/02/fair-selection.jpg?w=584\">\n\n\n### What is Fairness in Machine Learning?\nFairness in machine learning is all about ensuring that your model doesn’t produce biased results across different demographic groups, such as race, gender, or age. A fair model should provide equal performance and treatment for all groups, avoiding any form of discriminatory stereotypes. In other words, fairness means that your model’s decisions are just and unbiased, regardless of who it’s making predictions about.\n\n**Why Is Fairness Important?**\n\n* ***Ethical Considerations:***\n\nFairness in machine learning isn’t just a technical goal—it’s an ethical one. Unfair models can perpetuate and even amplify social biases, leading to discriminatory practices and unfair treatment of certain groups. This is why it’s crucial to ensure that our models don’t reinforce harmful stereotypes or biases that exist in the data.\n\n**When Should You Evaluate Fairness?**\n\n* ***High-Impact Decisions:*** \nIf your model is being used in scenarios where the stakes are high—like hiring decisions, loan approvals, law enforcement, or healthcare—it’s essential to evaluate fairness. These are situations where biased outcomes can have serious consequences for people’s lives.\n\n* ***Sensitive Attributes in Your Dataset:***\nWhenever your dataset includes sensitive attributes (like race, gender, or age) that could lead to biased results, you need to be vigilant about fairness. It’s important to check whether these attributes are causing the model to make unfair predictions.\n\n* ***Deployment in Diverse Environments:***\nIf your model is going to be used in various environments where fairness matters across different groups, it’s crucial to ensure it performs fairly for everyone.\n\n**What to Consider When Choosing an Algorithm:**\n\n* ***Some Algorithms Are More Prone to Bias:***\nNot all algorithms are created equal when it comes to fairness. For example, simpler models like decision trees or linear models can easily pick up on biases present in the training data, which can lead to biased predictions. On the other hand, more complex models like neural networks might learn subtle patterns that include biased behaviors, even if they aren’t immediately obvious.\n\n* ***Interpretable Models for Fairness:***\nUsing interpretable models, such as decision trees or logistic regression, can make it easier to spot and address biases. These models are more transparent, allowing you to understand how decisions are being made and identify any potential issues. In contrast, complex models like deep neural networks are more of a black box and can be harder to interpret, making it challenging to ensure fairness.\n\n\n\nUnderstanding both model robustness and fairness is crucial for developing reliable and ethical machine learning systems. By considering these factors, we can ensure our models perform consistently in the real world and make fair decisions for everyone—especially in scenarios where the impact on individuals is significant.","metadata":{}},{"cell_type":"code","source":"\nDemographic_feature = [\"FastingBS\",\"Sex_M\",\"ChestPainType_ATA\",\"ChestPainType_NAP\",\n                     \"ChestPainType_TA\",\"RestingECG_Normal\",\"RestingECG_ST\",\n                     \"ExerciseAngina_Y\",\"ST_Slope_Flat\",\"ST_Slope_Up\"]\nselected_models = {\n    'CatBoostClassifier': models['CatBoostClassifier'],\n    'xgb_model': models['XGBoost']\n}\n\n# Create a list to store the results\nall_results = []\n\nfor model_name, model_ in selected_models.items():\n        \n    for feature in Demographic_feature:\n\n        y_pred_tree = model_.predict(X_test_tree)\n\n        # Calculate fairness metrics using Fairlearn MetricFrame\n        sensitive_feature_index = X_train_tree.columns.get_loc(feature)\n        sensitive_feature = X_test_tree.iloc[:, sensitive_feature_index]\n\n        metric_frame = MetricFrame(\n            metrics={\"recall\": recall_score, \"selection_rate\": selection_rate},\n            y_true=y_test_tree,\n            y_pred=y_pred_tree,\n            sensitive_features=sensitive_feature\n        )\n\n        # Calculate and store differences between groups for Recall and Selection Rate\n        group_labels = sensitive_feature.unique()\n        group_metrics = []\n\n        for label in group_labels:\n            recall_value = metric_frame.by_group.loc[label, 'recall']\n            selection_rate_value = metric_frame.by_group.loc[label, 'selection_rate']\n            group_metrics.append({\n                \"Model\": model_name,\n                \"Feature\": feature,\n                \"Group\": f\" {label}\",\n                \"Recall\": recall_value,\n                \"Selection Rate\": selection_rate_value\n            })\n\n        # Add Demographic Parity Difference and Equalized Odds Difference metrics\n        dp_diff = demographic_parity_difference(y_test_tree, y_pred_tree, sensitive_features=sensitive_feature)\n        eo_diff = equalized_odds_difference(y_test_tree, y_pred_tree, sensitive_features=sensitive_feature)\n\n        group_metrics.append({\n            \"Model\": model_name,\n            \"Feature\": feature,\n            \"Group\": \"Demographic Parity Difference\",\n            \"Recall\": dp_diff,\n            \"Selection Rate\": None\n        })\n\n        group_metrics.append({\n            \"Model\": model_name,\n            \"Feature\": feature,\n            \"Group\": \"Equalized Odds Difference\",\n            \"Recall\": eo_diff,\n            \"Selection Rate\": None\n        })\n\n        # Store all results collectively\n        all_results.extend(group_metrics)\n        \n\n# Convert the results to a DataFrame\nresults_df = pd.DataFrame(all_results)\n\n# View DataFrame\nresults_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:16.920870Z","iopub.execute_input":"2024-08-26T23:19:16.921398Z","iopub.status.idle":"2024-08-26T23:19:18.363747Z","shell.execute_reply.started":"2024-08-26T23:19:16.921340Z","shell.execute_reply":"2024-08-26T23:19:18.362430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**General Evaluation: Fairness Analysis of Your Models**\n\nWhen we evaluate the fairness of your **CatBoost model** across different demographic features, we notice some potential biases that could be a cause for concern. Here are the key takeaways:\n\n***ST_Slope_Up and ST_Slope_Flat:***\n\nThese features show the most significant differences in Demographic Parity Difference and Equalized Odds Difference. What does that mean? Well, it suggests that our model is showing a significant bias against groups characterized by these features. In other words, the model's predictions are not as fair as they could be for these groups.\n\n***ExerseAngina_Y and ChestPainType_ATA:***\n\nWe also see notable fairness disparities here. Both the Demographic Parity Difference and Equalized Odds Difference are high, which indicates that the model might be making biased predictions against certain groups associated with these features.\n\n***FastingBS, Sex_M, and RestingECG_Normal:***\n\nOn a brighter note, the fairness differences for these features are relatively low. This suggests that the model is performing more consistently and fairly for groups with these characteristics, which is a good sign!\n\nNow, let’s take a look at the fairness analysis of your **XGBoost model:**\n\n***ST_Slope_Flat and ST_Slope_Up:***\n\nJust like with CatBoost, these features show the highest disparities in Demographic Parity Difference and Equalized Odds Difference. This indicates a strong bias against groups with these characteristics, signaling serious fairness issues within the model.\n\n***ExerciseAngina_Y and ChestPainType_ATA:***\n\nThese features also show significant fairness differences in the XGBoost model. Again, the high values for Demographic Parity Difference and Equalized Odds Difference suggest that the model may be biased against some groups associated with these features.\n\n***FastingBS, Sex_M, and RestingECG_Normal:***\n\nSimilar to the CatBoost model, the fairness differences for these features are relatively low. This indicates that the model is behaving more consistently and fairly across groups with these characteristics.","metadata":{}},{"cell_type":"markdown","source":"# Model Interpretability and Feature Importance\n\n\n\n\n<img src=\"https://www.researchgate.net/publication/381518989/figure/fig2/AS:11431281252656108@1718783012097/Feature-importance-for-model-interpretability-The-present-figure-represents-the.png\">\n\n","metadata":{}},{"cell_type":"markdown","source":"## Feature Importance\n\nFeature importance is all about understanding which features are most influential in making predictions with your model. In decision tree-based models, for instance, feature importance is often determined by counting how many times a feature is used to make a decision (like how often a feature splits the data into branches).\n\nHowever, feature importance has its limitations. While it tells you which features are generally more important for the model, it doesn’t give you the full picture. It doesn’t reveal details about interactions between features or show exactly how each feature contributes to individual predictions.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Model Interpretability with SHAP\n\nEnter SHAP values—a more advanced way to interpret model predictions. SHAP (SHapley Additive exPlanations) values provide a detailed look at the contribution of each feature to a specific model prediction. They’re based on Shapley values from cooperative game theory, which allow the effect of each feature on the prediction to be fairly distributed among all features.\n\nWhat’s cool about SHAP values is that they show exactly how much a model's prediction increases or decreases when a feature’s value changes. This gives you a clear understanding of how each feature is impacting the model’s decisions.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### Differences Between Feature Importance and SHAP Values\n\n* **Feature Importance:**\n\n  * Shows which features are more important overall.\n  * Provides a general sense of which features the model relies on the most, but it doesn’t indicate the direction of the impact or specific contributions to individual predictions.\n* **SHAP Values:**\n\n  * Show the effect of each feature on a particular prediction made by the model.\n  * Provide a detailed view of how each feature positively or negatively affects a prediction and by how much.\n  \n### Level of Detail:\n\n* **Feature Importances:**\n  * Tell you how much features are used overall in the model but don’t specify in what direction they affect the predictions.\n* **SHAP Values:**\n  * Indicate whether a feature has a positive or negative impact on predictions and quantify that effect.\n  \n  \n### Interactions:\n\n* **Feature Importances:**\n\n  * Typically do not account for interactions between features. They consider each feature independently, without showing how features might work together to affect predictions.\n* **SHAP Values:**\n\n  * Take feature interactions into account and demonstrate how multiple features can interact to influence the model’s predictions.\n  \n  \n*In summary, while feature importance gives a high-level overview of which features are crucial to the model, SHAP values go a step further by showing the detailed impact of each feature, including interactions and directionality. This makes SHAP a powerful tool for understanding and interpreting complex models, especially when you need to explain model behavior in detail.*","metadata":{}},{"cell_type":"code","source":"xgb_model = models['XGBoost']\n\n# Calculate feature importance levels\nimportance = xgb_model.get_booster().get_score(importance_type='weight')\n\n# Convert and sort feature importances to a DataFrame\nimportance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False)\n\n# Visualize feature importance levels\nplt.figure(figsize=(10, 8))\nxgb.plot_importance(xgb_model, importance_type='weight', max_num_features=15)  # En önemli 10 özelliği gösterir\nplt.title('Feature Importance (Weight)')\nplt.show()\n\n# If you want to evaluate based on profit or coverage:\n# importance_gain = xgb_model.get_booster().get_score(importance_type='gain')\n# importance_cover = xgb_model.get_booster().get_score(importance_type='cover')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:18.365573Z","iopub.execute_input":"2024-08-26T23:19:18.366107Z","iopub.status.idle":"2024-08-26T23:19:18.712567Z","shell.execute_reply.started":"2024-08-26T23:19:18.366047Z","shell.execute_reply":"2024-08-26T23:19:18.711077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_train_tree)\n\n# Visualize the overall impact of features\nshap.summary_plot(shap_values, X_train_tree)\n\n# Analysis of SHAP values ​​by feature\nshap.dependence_plot(\"ST_Slope_Up\", shap_values, X_train_tree)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:18.713958Z","iopub.execute_input":"2024-08-26T23:19:18.714377Z","iopub.status.idle":"2024-08-26T23:19:20.258263Z","shell.execute_reply.started":"2024-08-26T23:19:18.714327Z","shell.execute_reply":"2024-08-26T23:19:20.256824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncatboost_model = models['CatBoostClassifier']\n\n# Calculating feature importance levels\nfeature_importance = catboost_model.get_feature_importance()\nfeatures = X_train_tree.columns\n\n# Convert and sort features and their importance into a DataFrame\nimportance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Visualize ranked feature importance levels\nplt.figure(figsize=(10, 8))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importance (CatBoost)')\nplt.gca().invert_yaxis()  \nplt.show()\n\n# Display the sorted table\nimportance_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:20.259918Z","iopub.execute_input":"2024-08-26T23:19:20.260361Z","iopub.status.idle":"2024-08-26T23:19:20.600400Z","shell.execute_reply.started":"2024-08-26T23:19:20.260320Z","shell.execute_reply":"2024-08-26T23:19:20.599081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating SHAP values\nexplainer = shap.TreeExplainer(catboost_model)\nshap_values = explainer.shap_values(X_train_tree)\n\n# Visualize the overall impact of features\nshap.summary_plot(shap_values, X_train_tree)\n\n# Analysis of SHAP values ​​by attribute (e.g. 'Age' attribute)\nshap.dependence_plot(\"Age\", shap_values, X_train_tree)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:20.606222Z","iopub.execute_input":"2024-08-26T23:19:20.606670Z","iopub.status.idle":"2024-08-26T23:19:22.197026Z","shell.execute_reply.started":"2024-08-26T23:19:20.606629Z","shell.execute_reply":"2024-08-26T23:19:22.195708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Probability Calibration\n\n<img src=\"https://ploomber.io/images/blog/calibration-curve/serialized/25-0.png\">","metadata":{}},{"cell_type":"markdown","source":"### What is Probability Calibration?\n​\nProbability calibration is a technique used to adjust the predicted probabilities of a machine learning model to better match the true probabilities of the outcomes. This process is especially important in scenarios where accurate probability estimates are crucial—like in emerging or high-stakes problems. The goal is to ensure that the model’s predicted probabilities are reliable and aligned with reality.\n​\n### Why is Probability Calibration Important?\n​\nIn many applications, it’s not just about making the right prediction but also about understanding how confident the model is in that prediction. For example, in a medical diagnosis scenario, knowing whether a model is 90% or 50% confident in predicting a disease can make a huge difference in decision-making. Similarly, in financial forecasting or risk assessment, calibrated probabilities can provide more accurate insights.\n​\n### When is Probability Calibration Done?\n​\n1. After the Model is Trained:\nOnce your model has been trained and you’ve evaluated its initial performance, you might notice that the predicted probabilities aren’t quite aligning with the real-world outcomes. This is when you perform probability calibration to fine-tune these estimates, making them more representative of true probabilities.\n​\n2. Performance Evaluation: \nAfter analyzing various performance metrics such as the Brier score, you may determine that your model’s probability estimates are inaccurate. If these metrics indicate that the model’s estimates are not as reliable or consistent as they should be, calibration can be applied to improve the accuracy of these probability estimates.\n​\n3. Especially in Cases of Imbalanced Data:\nIf your training data is imbalanced—say, there’s a significant disparity between the number of instances in different classes—your model’s probability estimates might be biased. For example, if one class is underrepresented, the model might not predict probabilities accurately for that class. In such cases, calibration is crucial to ensure fair and balanced probability estimates.\n​\n### How is Probability Calibration Useful?\n​\n* **Improving Decision-Making:** Calibrated probabilities help make better-informed decisions in critical applications, such as healthcare, finance, or any field where precise probability estimates are necessary.\n​\n* **Enhancing Model Reliability:** By ensuring that the predicted probabilities reflect real-world chances more accurately, calibration increases the trustworthiness of the model’s predictions.\n​\n* **Addressing Bias in Predictions:** When dealing with imbalanced datasets, calibration can help correct biases in probability estimates, making the model’s outputs fairer and more accurate.\n​\n*In summary, probability calibration is a valuable step in fine-tuning a machine learning model’s performance, especially when you need reliable probability estimates. It ensures that the model’s predictions are not only correct but also appropriately confident, leading to better decisions and more trustworthy outcomes.*","metadata":{}},{"cell_type":"code","source":"\n# Calibrate the model\ncalibrated_model = CalibratedClassifierCV(base_estimator=catboost_model, method='sigmoid', cv=5)\ncalibrated_model.fit(X_train_tree, y_train_tree)\n\n# Post-calibration estimates\ny_prob = calibrated_model.predict_proba(X_test_tree)[:, 1]\n\n# Measuring calibration success with Brier Score Loss\nbrier_loss = brier_score_loss(y_test_tree, y_prob)\nprint(f\"Brier Score Loss: {brier_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:22.198807Z","iopub.execute_input":"2024-08-26T23:19:22.199246Z","iopub.status.idle":"2024-08-26T23:19:24.372262Z","shell.execute_reply.started":"2024-08-26T23:19:22.199201Z","shell.execute_reply":"2024-08-26T23:19:24.370878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calibrate the model\ncalibrated_model = CalibratedClassifierCV(base_estimator=xgb_model, method='sigmoid', cv=5)\ncalibrated_model.fit(X_train_tree, y_train_tree)\n\n# Post-calibration estimates\ny_prob = calibrated_model.predict_proba(X_test_tree)[:, 1]\n\n# Measuring calibration success with Brier Score Loss\nbrier_loss = brier_score_loss(y_test_tree, y_prob)\nprint(f\"Brier Score Loss: {brier_loss}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:24.373853Z","iopub.execute_input":"2024-08-26T23:19:24.374370Z","iopub.status.idle":"2024-08-26T23:19:24.766772Z","shell.execute_reply.started":"2024-08-26T23:19:24.374310Z","shell.execute_reply":"2024-08-26T23:19:24.765825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's perform the calibration\n\ncalibrated_catboost = CalibratedClassifierCV(base_estimator=catboost_model, method='sigmoid', cv=5)\ncalibrated_catboost.fit(X_train_tree, y_train_tree)\n\n# Post-calibration estimates\ny_prob_calibrated = calibrated_catboost.predict_proba(X_test_tree)[:, 1]\n\n# Pre-calibration estimates\ny_prob_uncalibrated = catboost_model.predict_proba(X_test_tree)[:, 1]\n\n# Let's calculate the Brier scores\nbrier_uncalibrated = brier_score_loss(y_test_tree, y_prob_uncalibrated)\nbrier_calibrated = brier_score_loss(y_test_tree, y_prob_calibrated)\n\nprint(f\"Brier Score (Uncalibrated): {brier_uncalibrated:.4f}\")\nprint(f\"Brier Score (Calibrated): {brier_calibrated:.4f}\")\n\n\n# Let's draw the calibration curve\nplt.figure(figsize=(10, 8))\n\n# Before calibration\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test_tree, y_prob_uncalibrated, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Uncalibrated\")\n\n# After calibration\nfraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(y_test_tree, y_prob_calibrated, n_bins=10)\nplt.plot(mean_predicted_value_cal, fraction_of_positives_cal, \"s-\", label=\"Calibrated (Isotonic)\")\n\n# Perfect calibration line\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n\nplt.xlabel(\"Mean predicted value\")\nplt.ylabel(\"Fraction of positives\")\nplt.title(\"Calibration Curve\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:24.768626Z","iopub.execute_input":"2024-08-26T23:19:24.770358Z","iopub.status.idle":"2024-08-26T23:19:27.357347Z","shell.execute_reply.started":"2024-08-26T23:19:24.770301Z","shell.execute_reply":"2024-08-26T23:19:27.356009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalized Model\n","metadata":{}},{"cell_type":"markdown","source":"### Data Splitting","metadata":{}},{"cell_type":"code","source":"X = df_encoded.drop('HeartDisease', axis=1)\ny = df_encoded['HeartDisease']\n\n# Let's split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Let's check the sizes of training and test sets\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:27.359094Z","iopub.execute_input":"2024-08-26T23:19:27.359604Z","iopub.status.idle":"2024-08-26T23:19:27.376090Z","shell.execute_reply.started":"2024-08-26T23:19:27.359549Z","shell.execute_reply":"2024-08-26T23:19:27.374652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalization and Standardization: What’s the Difference?\n\nWhen working with machine learning models, you might often hear about normalization and standardization. These are two common techniques used to rescale data, but they’re used in different scenarios and for different reasons. Let’s break down what each one means and when you might want to use them.\n\n### Normalization\n\n* **Definition:** Normalization is the process of rescaling your data to a specific range, usually between 0 and 1. This technique is handy when you want to make sure all your data points fall within a uniform range.\n\n* **How to Do It:** To normalize your data, you adjust each data point based on its minimum and maximum values. Essentially, you subtract the minimum value of the data from each data point and then divide by the range (maximum value minus minimum value). This way, all your data ends up on the same scale.\n\n* **When to Use It:** If your dataset has features with a wide range of values or if the data distribution isn’t normal (like when it’s positively skewed), normalization can be very useful. It helps bring all the data into a comparable range, which can improve the performance of some machine learning algorithms.\n\n### Standardization\n\n* **Definition:** Standardization, on the other hand, involves rescaling your data so that it has a mean of 0 and a standard deviation of 1. This process adjusts the data to have a normal distribution (or at least, a distribution centered around zero).\n\n* **How to Do It:** To standardize your data, you subtract the mean of the data from each data point and then divide by the standard deviation. This results in data that is centered around zero with a consistent scale.\n\n* **When to Use It:** Standardization is often the go-to option if your data is generally normally distributed or when you’re using algorithms that assume a normal distribution. It helps ensure that the data is centered and has a uniform variance, which can be crucial for many statistical models.\n\n### So, Which One Should You Choose?\n\nHonestly, there’s no one-size-fits-all answer here, friends. The choice between normalization and standardization depends on your specific dataset and the requirements of your machine learning model. Some algorithms are more sensitive to the range and distribution of the data, while others are not.\n\nIf you’re unsure, there’s no harm in trying both! Experiment with normalization and standardization separately and see which one works best for your model. Sometimes, the best way to find out is through a bit of trial and error.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:27.378006Z","iopub.execute_input":"2024-08-26T23:19:27.378459Z","iopub.status.idle":"2024-08-26T23:19:27.398105Z","shell.execute_reply.started":"2024-08-26T23:19:27.378392Z","shell.execute_reply":"2024-08-26T23:19:27.396619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define the models\nmodels_non_tree = {    \n    'SVC': SVC(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=5),\n    \"LogisticRegression\": LogisticRegression(random_state=42),\n    'RidgeClassifier': RidgeClassifier(random_state=42),\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:27.400197Z","iopub.execute_input":"2024-08-26T23:19:27.400710Z","iopub.status.idle":"2024-08-26T23:19:27.408392Z","shell.execute_reply.started":"2024-08-26T23:19:27.400653Z","shell.execute_reply":"2024-08-26T23:19:27.406872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef optimize_support_vector_classifier(X_train_scaled, y_train):\n    param_dist_svc = {\n    'C': [0.1, 1, 10, 100, 1000],\n    'gamma': [1e-3, 1e-4, 'scale', 'auto'],\n    'kernel': ['linear', 'rbf']\n}\n    halving_search_svc = HalvingRandomSearchCV(SVC(random_state=42), param_dist_svc, factor=3, cv=5, random_state=42)\n    halving_search_svc.fit(X_train_scaled, y_train)\n    return halving_search_svc.best_estimator_\n\n\ndef optimize_logistic(X_train_scaled, y_train):\n    param_grid_logistic = {\n    'C': (1e-6, 1e+6, 'log-uniform'),\n    'penalty': ['l2'],\n    'solver': ['liblinear', 'saga']\n}\n    bayes_search_logistic = BayesSearchCV(LogisticRegression(random_state=42), param_grid_logistic, n_iter=30, cv=5, random_state=42)\n    bayes_search_logistic.fit(X_train_scaled, y_train)\n    return bayes_search_logistic.best_estimator_\n\ndef optimize_knn(X_train_scaled, y_train):\n    param_dist_knn = {\n    'n_neighbors': range(1, 31),\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan', 'minkowski']\n}\n    random_search_knn = RandomizedSearchCV(KNeighborsClassifier(), param_dist_knn, n_iter=30, cv=5, random_state=42)\n    random_search_knn.fit(X_train_scaled, y_train)\n    return random_search_knn.best_estimator_\n\ndef optimize_ridge(X_train_scaled, y_train):\n    param_grid_ridge = {\n    'alpha': [0.1, 1.0, 10.0, 100.0],\n    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag']\n}\n\n    grid_search_ridge = GridSearchCV(RidgeClassifier(random_state=42), param_grid_ridge, cv=5)\n    grid_search_ridge.fit(X_train_scaled, y_train)\n    return grid_search_ridge.best_estimator_\n\n\nmodels_non_tree['SVC'] = optimize_support_vector_classifier(X_train_scaled, y_train)\nmodels_non_tree['KNeighborsClassifier'] = optimize_knn(X_train_scaled, y_train)\nmodels_non_tree['LogisticRegression'] = optimize_logistic(X_train_scaled, y_train)\nmodels_non_tree['RidgeClassifier'] = optimize_ridge(X_train_scaled, y_train)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:27.410611Z","iopub.execute_input":"2024-08-26T23:19:27.411164Z","iopub.status.idle":"2024-08-26T23:19:59.096025Z","shell.execute_reply.started":"2024-08-26T23:19:27.411105Z","shell.execute_reply":"2024-08-26T23:19:59.094324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_non_tree.items()","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:59.098282Z","iopub.execute_input":"2024-08-26T23:19:59.099203Z","iopub.status.idle":"2024-08-26T23:19:59.109615Z","shell.execute_reply.started":"2024-08-26T23:19:59.099142Z","shell.execute_reply":"2024-08-26T23:19:59.108288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lists to store results\nrecall_scores_test = []\nrecall_scores_train = []\nmodel_names = []\n\n# Get recall scores for each model\nfor name, model in models_non_tree.items():\n    \n    # Make a prediction (Using trained models in hyperparameter optimization)\n    y_pred = model.predict(X_test_scaled)\n    y_pred_tr = model.predict(X_train_scaled)\n    \n    # Calculate and save recall scores\n    recall_test = recall_score(y_test, y_pred, average='macro')\n    recall_train = recall_score(y_train, y_pred_tr, average='macro')\n    \n    recall_scores_test.append(recall_test)\n    recall_scores_train.append(recall_train)\n    model_names.append(name)\n\n# Let's visualize the recall scores on the same graph\nplt.figure(figsize=(12, 6))\n\nplt.plot(model_names, recall_scores_train, marker='o', linestyle='-', color='b', label='Train Recall')\nplt.plot(model_names, recall_scores_test, marker='o', linestyle='-', color='r', label='Test Recall')\n\nplt.xlabel('Models')\nplt.ylabel('Recall')\nplt.title('Train and Test Recall for Different Models')\nplt.xticks(rotation=45)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:59.111854Z","iopub.execute_input":"2024-08-26T23:19:59.112802Z","iopub.status.idle":"2024-08-26T23:19:59.619750Z","shell.execute_reply.started":"2024-08-26T23:19:59.112735Z","shell.execute_reply":"2024-08-26T23:19:59.618443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_data = []\n\nfor name, model in models_non_tree.items():\n        \n    # Make a guess\n    y_pred = model.predict(X_test_scaled)\n    y_pred_tr = model.predict(X_train_scaled)\n    \n    # Classification report'u al\n    report_test = classification_report(y_test, y_pred, output_dict=True)\n    report_train = classification_report(y_train, y_pred_tr, output_dict=True)\n    \n    # Let's tabulate the classification report values ​​for each class\n    for label in report_test.keys():\n        if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n            comparison_data.append({\n                'Model': name,\n                'DataSet': 'Test',\n                'Label': label,\n                'Precision': report_test[label]['precision'],\n                'Recall': report_test[label]['recall'],\n                'F1-Score': report_test[label]['f1-score'],\n                'Accuracy': report_test['accuracy']  # Accuracy değerini ekliyoruz\n            })\n            comparison_data.append({\n                'Model': name,\n                'DataSet': 'Train',\n                'Label': label,\n                'Precision': report_train[label]['precision'],\n                'Recall': report_train[label]['recall'],\n                'F1-Score': report_train[label]['f1-score'],\n                'Accuracy': report_train['accuracy']  # Accuracy değerini ekliyoruz\n            })\n\n# Convert the results to a DataFrame\ncomparison_df = pd.DataFrame(comparison_data)\n\n\n\n# Let's view the DataFrame\ncomparison_df\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:59.621422Z","iopub.execute_input":"2024-08-26T23:19:59.621864Z","iopub.status.idle":"2024-08-26T23:19:59.802448Z","shell.execute_reply.started":"2024-08-26T23:19:59.621818Z","shell.execute_reply":"2024-08-26T23:19:59.801217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_recall_test = max(comparison_data, key=lambda x: x['Recall'] if (x['DataSet'] == 'Test' and x['Recall']<1) else 0)\n\nprint(\"The best model based test on recall is:\", best_model_recall_test['Model'])\nprint(\"Recall:\", best_model_recall_test['Recall'])\n\nbest_model_recall_train = max(comparison_data, key=lambda x: x['Recall'] if (x['DataSet'] == 'Train' and x['Recall']<1) else 0)\n\nprint(\"The best model based on train recall is:\", best_model_recall_train['Model'])\nprint(\"Recall:\", best_model_recall_train['Recall'])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:59.804588Z","iopub.execute_input":"2024-08-26T23:19:59.805372Z","iopub.status.idle":"2024-08-26T23:19:59.818595Z","shell.execute_reply.started":"2024-08-26T23:19:59.805306Z","shell.execute_reply":"2024-08-26T23:19:59.817041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's train a linear SVC model\nmodel_svc = models_non_tree['SVC']\nmodel_svc.fit(X_train_scaled, y_train)\n\n# Let's calculate feature importances\nfeature_importances = abs(model_svc.coef_[0])\nimportance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\n# Visualization\nplt.figure(figsize=(10, 8))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\nplt.xlabel('Coefficient Value (Importance)')\nplt.title('Feature Importance for Linear SVC')\nplt.gca().invert_yaxis()  # En önemli özelliklerin üstte olması için ters çeviriyoruz\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:19:59.821082Z","iopub.execute_input":"2024-08-26T23:19:59.822099Z","iopub.status.idle":"2024-08-26T23:20:00.204877Z","shell.execute_reply.started":"2024-08-26T23:19:59.822010Z","shell.execute_reply":"2024-08-26T23:20:00.203609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LIME (Local Interpretable Model-agnostic Explanations)\n\n**What is LIME?**\n\nLIME stands for Local Interpretable Model-agnostic Explanations. It’s a tool designed to explain individual predictions of any machine learning model, making it incredibly versatile and powerful. The cool thing about LIME is that it’s model-agnostic, which means it doesn’t matter what type of machine learning model you’re using—LIME can work with all of them!\n\n**How Does LIME Work?**\n\nLIME explains a model's decision for a specific prediction by creating a simple, interpretable model (usually a linear model) that approximates the complex model locally around the prediction. In other words, LIME focuses on one prediction at a time, building a local explanation that helps you understand why the model made that particular prediction.\n\nThink of LIME as creating a small, simplified snapshot of your model's decision-making process for just one data point. It essentially says, \"Hey, if we look at this prediction closely and simplify things a bit, here’s how the model is making its decision.\" This makes it easier to understand and trust the model’s predictions, especially when dealing with complex or black-box models.\n\n**Key Features of LIME:**\n\n* **Model-agnostic:** Works with any machine learning model, whether it's a simple logistic regression or a complex deep neural network.\n\n* **Local Explanations:** Focuses on explaining individual predictions rather than the overall model. This makes it great for understanding specific decisions and gaining insights into why a model behaved in a certain way for a particular instance.\n\n* **Interpretable Models:** LIME builds a simple, interpretable model (like a linear model) around the prediction to approximate the complex model’s behavior locally. This helps in visualizing and understanding the influence of each feature on that specific prediction.\n\nIn summary, LIME is a fantastic tool when you need to interpret the decisions of your machine learning model on a granular, case-by-case basis. It’s especially useful when working with complex models where understanding the rationale behind each prediction can provide valuable insights and improve trust in the model’s outputs.","metadata":{}},{"cell_type":"code","source":"# Let's train the SVC model\nmodel_svc = SVC(C=1, kernel='linear',probability=True, random_state=42)\n# For LIME to use predict_proba, probability=True\nmodel_svc.fit(X_train_scaled, y_train)\n\n# Create the LIME descriptor\nexplainer = lime.lime_tabular.LimeTabularExplainer(\n    training_data=X_train_scaled,\n    feature_names=X_train.columns,\n    class_names=['No HeartDisease', 'HeartDisease'],  # Sınıf adlarını buraya koyabilirsiniz\n    mode='classification'\n)\n\n# Select sample data to explain a single prediction\ni = 0  # İlk örneği seçiyoruz, başka bir index seçebilirsiniz\nexp = explainer.explain_instance(X_test_scaled[i], model_svc.predict_proba, num_features=10)\n\n# Visualize feature importance\nexp.show_in_notebook(show_table=True)\n\nfig = exp.as_pyplot_figure()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:20:00.206663Z","iopub.execute_input":"2024-08-26T23:20:00.207114Z","iopub.status.idle":"2024-08-26T23:20:00.834826Z","shell.execute_reply.started":"2024-08-26T23:20:00.207069Z","shell.execute_reply":"2024-08-26T23:20:00.833461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's perform the calibration\n\ncalibrated_svc = CalibratedClassifierCV(base_estimator=model_svc, method='isotonic', cv=5)\ncalibrated_svc.fit(X_train_scaled, y_train)\n\n# Post-calibration estimates\ny_prob_calibrated = calibrated_svc.predict_proba(X_test_scaled)[:, 1]\n\n# Pre-calibration estimates\ny_prob_uncalibrated = model_svc.predict_proba(X_test_scaled)[:, 1]\n\n# Let's calculate the Brier scores\nbrier_uncalibrated = brier_score_loss(y_test, y_prob_uncalibrated)\nbrier_calibrated = brier_score_loss(y_test, y_prob_calibrated)\n\nprint(f\"Brier Score (Uncalibrated): {brier_uncalibrated:.4f}\")\nprint(f\"Brier Score (Calibrated): {brier_calibrated:.4f}\")\n\n\n# Let's draw the calibration curve\nplt.figure(figsize=(10, 8))\n\n# Before calibration\nfraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_prob_uncalibrated, n_bins=10)\nplt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Uncalibrated\")\n\n# After calibration\nfraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(y_test, y_prob_calibrated, n_bins=10)\nplt.plot(mean_predicted_value_cal, fraction_of_positives_cal, \"s-\", label=\"Calibrated (Isotonic)\")\n\n\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\")\n\nplt.xlabel(\"Mean predicted value\")\nplt.ylabel(\"Fraction of positives\")\nplt.title(\"Calibration Curve\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-26T23:20:00.836505Z","iopub.execute_input":"2024-08-26T23:20:00.837032Z","iopub.status.idle":"2024-08-26T23:20:01.537213Z","shell.execute_reply.started":"2024-08-26T23:20:00.836956Z","shell.execute_reply":"2024-08-26T23:20:01.535742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Robustness\nWhat is Robustness?\nRobustness in machine learning refers to a model’s ability to keep performing well even when the going gets tough. This means that when a model is exposed to challenging conditions—like noisy data, missing information, or even adversarial attacks—it still manages to produce reliable results. Essentially, a robust model doesn’t lose its cool when the input data isn’t perfect or when unexpected changes occur in the data distribution.\n\nWhy is Robustness Important in Real-World Applications?\nIn the real world, data is rarely clean or perfect. Models often have to deal with noise, outliers, or missing information, and this is where robustness becomes a key factor. A robust model handles these imperfect conditions gracefully without its performance taking a nosedive. Imagine a model deployed in a noisy environment or one that has to make predictions when some of the data is missing—robustness ensures it still gets the job done.\n\nAdaptability Matters\nA robust model is not just tough; it’s also adaptable. It can handle new data or slightly different datasets than what it was trained on. This adaptability is crucial for models that will be deployed in dynamic environments where the data can change unpredictably.\n\nWhen Should You Test for Robustness?\nDynamic Environments: If your model is going to be used in an environment where data might change unexpectedly, robustness testing is essential. You want to make sure your model can adapt to these changes without losing performance.\n\nNoisy or Missing Data: In situations where your model will have to deal with noisy or incomplete data, robustness testing helps ensure it can still function effectively.\n\nCritical Applications: For applications where even small errors can have significant consequences—like medical diagnostics or autonomous driving—robustness is not just a nice-to-have; it’s a necessity.\n\nThe Role of Dataset Size in Robustness\nLarge Datasets: Generally speaking, more data helps models generalize better, which can increase their robustness. However, it’s not just about quantity. The data also needs to be diverse and cover various scenarios the model might encounter. This diversity prepares the model for real-world challenges, making it more robust.\n\nSmall Datasets: On the flip side, models trained on small datasets may not be as robust. They are more likely to overfit the training data, meaning they perform well on what they’ve seen but struggle with new, unseen data. This lack of generalization can make them less robust in real-world applications.\n\nTesting Robustness: After Training, Before Deployment\nOnce your model is trained, it’s crucial to perform robustness testing before deployment. This involves putting your model through its paces to see how it handles different scenarios. You might add noise to the test data, remove certain features, or test the model on slightly different data distributions. By doing this, you can identify any weaknesses and ensure that your model is ready for whatever the real world throws at it.","metadata":{}},{"cell_type":"markdown","source":"After these steps, all that remains is to perform the model usage steps.","metadata":{}},{"cell_type":"markdown","source":"# Model Deploy\n# Post-Deployment Monitoring\n# Model Drift","metadata":{}},{"cell_type":"markdown","source":"> ***Thanks for sticking with me until the end! If you enjoyed this notebook, please don't forget to upvote it. And if you have any feedback or suggestions on things that could be added or improved, I’d love to hear from you. Your advice is always welcome!***\n\n<img src=\"https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExcGVrbGt0cnU0eXZycXdpbW5idGZueWtmOTN6YXRyY2x1cGdoczJzOCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/7zliaSiCjWREhN1iqC/giphy.webp\">","metadata":{}}]}